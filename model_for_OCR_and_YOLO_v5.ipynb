{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yashkesar/Licence_Plate_Number_Detection-using-yolov5/blob/main/model_for_OCR_and_YOLO_v5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ak7PG5jpHwhq",
        "outputId": "61d0fec7-9e1e-4291-b574-2179e6307cbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 15368, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 15368 (delta 2), reused 6 (delta 1), pack-reused 15355\u001b[K\n",
            "Receiving objects: 100% (15368/15368), 14.36 MiB | 18.57 MiB/s, done.\n",
            "Resolving deltas: 100% (10502/10502), done.\n",
            "/content/yolov5\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 KB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 KB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 KB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 KB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "#clone YOLOv5 and\n",
        "!git clone https://github.com/ultralytics/yolov5  # clone repo\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt # install dependencies\n",
        "%pip install -q roboflow\n",
        "\n",
        "import torch\n",
        "import os\n",
        "from IPython.display import Image, clear_output  # to display images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmTNZD5JIgGk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vnTqrc9ONTV",
        "outputId": "cff412d3-1da5-4214-e8dd-bb37d8fad89d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete. Using torch 1.13.1+cu116 (Tesla T4)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqB6AOhSOJsl"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"BACOwWHFwYUfKdmtjeJ0\")\n",
        "project = rf.workspace(\"srm-hybx5\").project(\"text-based-traffic-signboard-custom-dataset\")\n",
        "dataset = project.version(5).download(\"yolov5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZX8dtiJvNB-",
        "outputId": "10b8c34c-d88a-490c-cbc4-81544072cf9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: roboflow in /usr/local/lib/python3.9/dist-packages (1.0.2)\n",
            "Requirement already satisfied: chardet==4.0.0 in /usr/local/lib/python3.9/dist-packages (from roboflow) (4.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from roboflow) (2.27.1)\n",
            "Requirement already satisfied: certifi==2022.12.7 in /usr/local/lib/python3.9/dist-packages (from roboflow) (2022.12.7)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.9/dist-packages (from roboflow) (4.65.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.9/dist-packages (from roboflow) (1.0.0)\n",
            "Requirement already satisfied: pyparsing==2.4.7 in /usr/local/lib/python3.9/dist-packages (from roboflow) (2.4.7)\n",
            "Requirement already satisfied: opencv-python>=4.1.2 in /usr/local/lib/python3.9/dist-packages (from roboflow) (4.7.0.72)\n",
            "Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.9/dist-packages (from roboflow) (2.10)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.9/dist-packages (from roboflow) (1.4.4)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.9/dist-packages (from roboflow) (3.2)\n",
            "Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.9/dist-packages (from roboflow) (0.10.0)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.9/dist-packages (from roboflow) (0.10.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from roboflow) (3.7.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.9/dist-packages (from roboflow) (6.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.9/dist-packages (from roboflow) (2.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from roboflow) (1.16.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from roboflow) (1.22.4)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.9/dist-packages (from roboflow) (8.4.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.9/dist-packages (from roboflow) (1.26.15)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->roboflow) (4.39.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->roboflow) (1.0.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->roboflow) (23.0)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->roboflow) (5.12.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->roboflow) (2.0.12)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->roboflow) (3.15.0)\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Downloading Dataset Version Zip in Text-based-traffic-signboard-custom-dataset-5 to yolov5pytorch: 100% [23095348 / 23095348] bytes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Dataset Version Zip to Text-based-traffic-signboard-custom-dataset-5 in yolov5pytorch:: 100%|██████████| 1026/1026 [00:00<00:00, 2091.23it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --batch 16 --epochs 55 --data /content/yolov5/Text-based-traffic-signboard-custom-dataset-5/data.yaml --weights yolov5n.pt --cache --hyp hyp.scratch-med.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIvXdyH_vZos",
        "outputId": "0be9936c-df96-4351-e114-4469fe7c4f39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-06 18:17:54.666027: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-04-06 18:17:55.593318: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=/content/yolov5/Text-based-traffic-signboard-custom-dataset-5/data.yaml, hyp=hyp.scratch-med.yaml, epochs=55, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
            "YOLOv5 🚀 v7.0-134-g23c4923 Python-3.9.16 torch-2.0.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.3, cls_pw=1.0, obj=0.7, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.2, scale=0.9, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.15, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "100% 755k/755k [00:00<00:00, 32.1MB/s]\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n",
            "100% 14.1M/14.1M [00:00<00:00, 55.7MB/s]\n",
            "\n",
            "Overriding model.yaml nc=80 with nc=2\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     18879  models.yolo.Detect                      [2, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model summary: 214 layers, 7025023 parameters, 7025023 gradients, 16.0 GFLOPs\n",
            "\n",
            "Transferred 343/349 items from yolov5s.pt\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/yolov5/Text-based-traffic-signboard-custom-dataset-5/train/labels... 440 images, 2 backgrounds, 0 corrupt: 100% 440/440 [00:00<00:00, 1361.67it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/yolov5/Text-based-traffic-signboard-custom-dataset-5/train/labels.cache\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.5GB ram): 100% 440/440 [00:02<00:00, 161.90it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/yolov5/Text-based-traffic-signboard-custom-dataset-5/valid/labels... 40 images, 0 backgrounds, 0 corrupt: 100% 40/40 [00:00<00:00, 706.55it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/yolov5/Text-based-traffic-signboard-custom-dataset-5/valid/labels.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB ram): 100% 40/40 [00:00<00:00, 112.20it/s]\n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m3.88 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n",
            "Plotting labels to runs/train/exp/labels.jpg... \n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp\u001b[0m\n",
            "Starting training for 55 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       0/54      3.46G     0.1057    0.02274    0.01433         17        640: 100% 28/28 [00:12<00:00,  2.18it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:01<00:00,  1.04it/s]\n",
            "                   all         40         67       0.12     0.0549     0.0841     0.0231\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       1/54      4.27G    0.07783    0.02182   0.005602         21        640: 100% 28/28 [00:07<00:00,  3.74it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  2.93it/s]\n",
            "                   all         40         67      0.293      0.284      0.216     0.0725\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       2/54      4.27G    0.07508    0.01915   0.002913         19        640: 100% 28/28 [00:06<00:00,  4.54it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  2.53it/s]\n",
            "                   all         40         67      0.437      0.493      0.447      0.164\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       3/54      4.27G    0.06883    0.01944    0.00317         28        640: 100% 28/28 [00:07<00:00,  3.74it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  3.54it/s]\n",
            "                   all         40         67      0.443      0.567      0.517      0.202\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       4/54      4.27G     0.0601    0.01862   0.002998         23        640: 100% 28/28 [00:07<00:00,  3.99it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:01<00:00,  1.70it/s]\n",
            "                   all         40         67      0.492      0.606      0.497      0.203\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       5/54      4.27G    0.05568    0.01824     0.0025         27        640: 100% 28/28 [00:06<00:00,  4.51it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  3.36it/s]\n",
            "                   all         40         67      0.543      0.728       0.62      0.295\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       6/54      4.27G    0.05144    0.01617   0.002177         30        640: 100% 28/28 [00:07<00:00,  3.72it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  2.95it/s]\n",
            "                   all         40         67        0.8      0.731      0.806      0.356\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       7/54      4.27G    0.04747    0.01556   0.001956         21        640: 100% 28/28 [00:06<00:00,  4.29it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  3.63it/s]\n",
            "                   all         40         67      0.659      0.672      0.655      0.399\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       8/54      4.27G    0.04744    0.01406   0.001911         19        640: 100% 28/28 [00:07<00:00,  3.72it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  3.28it/s]\n",
            "                   all         40         67      0.724      0.703      0.774       0.37\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       9/54      4.27G    0.04396    0.01393   0.001838         21        640: 100% 28/28 [00:06<00:00,  4.42it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:01<00:00,  1.94it/s]\n",
            "                   all         40         67      0.792      0.795      0.766      0.478\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      10/54      4.27G    0.04371    0.01406     0.0018         24        640: 100% 28/28 [00:06<00:00,  4.42it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  3.43it/s]\n",
            "                   all         40         67       0.76      0.791      0.752      0.444\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      11/54      4.27G    0.04099    0.01444   0.001482         31        640: 100% 28/28 [00:07<00:00,  3.58it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  3.25it/s]\n",
            "                   all         40         67      0.756      0.821      0.797      0.501\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      12/54      4.27G    0.03836    0.01247   0.001548         22        640: 100% 28/28 [00:06<00:00,  4.28it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  3.56it/s]\n",
            "                   all         40         67       0.73      0.851      0.809      0.431\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      13/54      4.27G    0.03918    0.01258   0.001279         31        640: 100% 28/28 [00:07<00:00,  3.66it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  3.25it/s]\n",
            "                   all         40         67      0.735      0.761       0.77      0.493\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      14/54      4.27G    0.03956    0.01233    0.00125         24        640: 100% 28/28 [00:06<00:00,  4.56it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:01<00:00,  1.68it/s]\n",
            "                   all         40         67      0.699      0.791      0.785      0.512\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      15/54      4.27G    0.03828    0.01254   0.001264         29        640: 100% 28/28 [00:06<00:00,  4.17it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  3.51it/s]\n",
            "                   all         40         67       0.75      0.849      0.798      0.535\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      16/54      4.27G    0.03753     0.0119   0.001267         18        640: 100% 28/28 [00:07<00:00,  3.89it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  2.74it/s]\n",
            "                   all         40         67      0.753      0.821      0.753      0.521\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      17/54      4.27G    0.03578    0.01226   0.001417         21        640: 100% 28/28 [00:06<00:00,  4.42it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  3.87it/s]\n",
            "                   all         40         67      0.767      0.835       0.74      0.542\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      18/54      4.27G     0.0345    0.01169   0.001123         25        640: 100% 28/28 [00:07<00:00,  3.73it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  3.22it/s]\n",
            "                   all         40         67      0.671      0.836      0.749      0.508\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      19/54      4.27G    0.03402    0.01251  0.0009345         28        640: 100% 28/28 [00:06<00:00,  4.39it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  2.53it/s]\n",
            "                   all         40         67       0.64      0.849       0.77      0.518\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      20/54      4.27G    0.03489    0.01175   0.001094         19        640: 100% 28/28 [00:06<00:00,  4.02it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  3.45it/s]\n",
            "                   all         40         67      0.655      0.881      0.778      0.529\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      21/54      4.27G    0.03276    0.01107   0.001073         38        640: 100% 28/28 [00:06<00:00,  4.02it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  2.05it/s]\n",
            "                   all         40         67      0.753      0.836      0.818      0.589\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      22/54      4.27G     0.0331     0.0119   0.001086         27        640: 100% 28/28 [00:06<00:00,  4.51it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  3.26it/s]\n",
            "                   all         40         67      0.758      0.866      0.805      0.561\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      23/54      4.27G    0.03268    0.01159   0.001049         21        640: 100% 28/28 [00:07<00:00,  3.66it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  3.29it/s]\n",
            "                   all         40         67      0.753      0.836       0.81      0.551\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      24/54      4.27G    0.03183    0.01064  0.0008636         20        640: 100% 28/28 [00:06<00:00,  4.51it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  3.62it/s]\n",
            "                   all         40         67      0.727      0.821      0.826      0.607\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      25/54      4.27G    0.03196    0.01117  0.0009025         17        640: 100% 28/28 [00:07<00:00,  3.75it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  3.31it/s]\n",
            "                   all         40         67      0.816      0.862      0.817      0.603\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      26/54      4.27G     0.0317    0.01137  0.0008063         31        640: 100% 28/28 [00:06<00:00,  4.06it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:01<00:00,  1.82it/s]\n",
            "                   all         40         67       0.79      0.786      0.819      0.574\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      27/54      4.27G    0.03028    0.01106   0.001013         25        640: 100% 28/28 [00:06<00:00,  4.56it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  3.16it/s]\n",
            "                   all         40         67       0.75      0.881      0.821      0.613\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      28/54      4.27G    0.02985     0.0115  0.0008566         39        640: 100% 28/28 [00:07<00:00,  3.70it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  3.85it/s]\n",
            "                   all         40         67      0.791      0.836      0.845       0.58\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      29/54      4.27G    0.02996    0.01066  0.0007889         21        640: 100% 28/28 [00:06<00:00,  4.50it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  3.80it/s]\n",
            "                   all         40         67      0.791      0.848       0.83      0.653\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      30/54      4.27G    0.02879   0.009754  0.0008022         20        640: 100% 28/28 [00:07<00:00,  3.72it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  3.23it/s]\n",
            "                   all         40         67      0.777      0.834      0.813      0.611\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      31/54      4.27G    0.02892    0.01026  0.0007612         30        640: 100% 28/28 [00:06<00:00,  4.55it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  3.39it/s]\n",
            "                   all         40         67      0.765      0.836      0.823      0.613\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      32/54      4.27G    0.02925    0.01116  0.0006149         32        640: 100% 28/28 [00:07<00:00,  3.82it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  4.12it/s]\n",
            "                   all         40         67      0.793      0.881      0.851      0.637\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      33/54      4.27G    0.02909    0.01033  0.0005725         24        640: 100% 28/28 [00:06<00:00,  4.00it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:01<00:00,  1.91it/s]\n",
            "                   all         40         67      0.775      0.823       0.78      0.587\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      34/54      4.27G    0.02945     0.0103  0.0007466         14        640: 100% 28/28 [00:06<00:00,  4.35it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  3.67it/s]\n",
            "                   all         40         67      0.777      0.851      0.813      0.608\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      35/54      4.27G    0.02792   0.009878   0.001019         22        640: 100% 28/28 [00:07<00:00,  3.73it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  3.12it/s]\n",
            "                   all         40         67      0.773      0.851      0.829      0.618\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      36/54      4.27G    0.02681    0.00947  0.0006219         15        640: 100% 28/28 [00:06<00:00,  4.41it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  3.55it/s]\n",
            "                   all         40         67      0.765      0.881      0.836      0.644\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      37/54      4.27G    0.02832    0.01082  0.0005719         29        640: 100% 28/28 [00:07<00:00,  3.68it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  2.85it/s]\n",
            "                   all         40         67      0.809      0.881      0.859      0.646\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      38/54      4.27G     0.0267    0.01028  0.0006087         33        640: 100% 28/28 [00:06<00:00,  4.04it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  2.13it/s]\n",
            "                   all         40         67      0.818      0.836      0.857      0.636\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      39/54      4.27G    0.02762   0.009933  0.0005392         29        640: 100% 28/28 [00:06<00:00,  4.49it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  3.85it/s]\n",
            "                   all         40         67      0.777      0.851      0.805      0.616\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      40/54      4.27G    0.02598    0.01082  0.0006314         31        640: 100% 28/28 [00:07<00:00,  3.71it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  3.47it/s]\n",
            "                   all         40         67      0.752      0.806      0.817      0.623\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      41/54      4.27G    0.02506   0.009266  0.0006738         30        640: 100% 28/28 [00:06<00:00,  4.48it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  3.62it/s]\n",
            "                   all         40         67      0.736      0.821      0.832       0.65\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      42/54      4.27G    0.02637   0.009675  0.0005192         15        640: 100% 28/28 [00:07<00:00,  3.70it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  3.46it/s]\n",
            "                   all         40         67      0.768       0.84      0.819      0.649\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      43/54      4.27G    0.02618     0.0101  0.0006526         41        640: 100% 28/28 [00:06<00:00,  4.63it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  2.21it/s]\n",
            "                   all         40         67      0.815      0.806      0.806      0.629\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      44/54      4.27G    0.02527   0.009617   0.000645         22        640: 100% 28/28 [00:07<00:00,  3.86it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  3.51it/s]\n",
            "                   all         40         67      0.776      0.829      0.826      0.644\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      45/54      4.27G    0.02475   0.009805  0.0006726         14        640: 100% 28/28 [00:07<00:00,  3.89it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  2.09it/s]\n",
            "                   all         40         67      0.782      0.836       0.82      0.635\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      46/54      4.27G    0.02466   0.008962  0.0005866         40        640: 100% 28/28 [00:06<00:00,  4.56it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  4.65it/s]\n",
            "                   all         40         67        0.8      0.791      0.833      0.644\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      47/54      4.27G    0.02334    0.01032  0.0006497         31        640: 100% 28/28 [00:07<00:00,  3.75it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  3.24it/s]\n",
            "                   all         40         67      0.783      0.866      0.827       0.66\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      48/54      4.27G    0.02327   0.009109  0.0004937         14        640: 100% 28/28 [00:06<00:00,  4.41it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  3.74it/s]\n",
            "                   all         40         67      0.808       0.88      0.833      0.659\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      49/54      4.27G    0.02243   0.009301  0.0003181         25        640: 100% 28/28 [00:07<00:00,  3.74it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  3.07it/s]\n",
            "                   all         40         67      0.819      0.876      0.839      0.685\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      50/54      4.27G    0.02486   0.009496  0.0006637         28        640: 100% 28/28 [00:06<00:00,  4.21it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:01<00:00,  1.80it/s]\n",
            "                   all         40         67      0.821      0.866       0.84      0.675\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      51/54      4.27G    0.02422   0.009276  0.0005362         15        640: 100% 28/28 [00:06<00:00,  4.24it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  3.43it/s]\n",
            "                   all         40         67      0.777      0.885      0.842      0.674\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      52/54      4.27G     0.0238   0.009336  0.0007405         24        640: 100% 28/28 [00:07<00:00,  3.83it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  3.15it/s]\n",
            "                   all         40         67      0.838       0.85      0.841      0.672\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      53/54      4.27G    0.02349   0.009769  0.0006792         25        640: 100% 28/28 [00:06<00:00,  4.35it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  3.49it/s]\n",
            "                   all         40         67      0.797      0.879      0.831      0.666\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      54/54      4.27G    0.02277   0.009202  0.0005192         31        640: 100% 28/28 [00:07<00:00,  3.71it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  3.68it/s]\n",
            "                   all         40         67      0.814      0.836       0.83      0.661\n",
            "\n",
            "55 epochs completed in 0.124 hours.\n",
            "Optimizer stripped from runs/train/exp/weights/last.pt, 14.4MB\n",
            "Optimizer stripped from runs/train/exp/weights/best.pt, 14.4MB\n",
            "\n",
            "Validating runs/train/exp/weights/best.pt...\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  2.67it/s]\n",
            "                   all         40         67      0.819      0.876      0.838      0.684\n",
            "        text-signboard         40         67      0.819      0.876      0.838      0.684\n",
            "Results saved to \u001b[1mruns/train/exp\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python val.py --weights /content/yolov5/runs/train/exp/weights/best.pt --data  /content/yolov5/Text-based-traffic-signboard-custom-dataset-5/data.yaml --img 640"
      ],
      "metadata": {
        "id": "PZKKTD0OwV_d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93693d48-429f-4605-e919-8151d037ecdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mdata=/content/yolov5/Text-based-traffic-signboard-custom-dataset-5/data.yaml, weights=['/content/yolov5/runs/train/exp/weights/best.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=False, dnn=False\n",
            "YOLOv5 🚀 v7.0-134-g23c4923 Python-3.9.16 torch-2.0.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/yolov5/Text-based-traffic-signboard-custom-dataset-5/valid/labels.cache... 40 images, 0 backgrounds, 0 corrupt: 100% 40/40 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:01<00:00,  1.25it/s]\n",
            "                   all         40         67      0.819      0.876      0.839      0.685\n",
            "        text-signboard         40         67      0.819      0.876      0.839      0.685\n",
            "Speed: 0.2ms pre-process, 12.7ms inference, 2.9ms NMS per image at shape (32, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/val/exp\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Frvw2FPNRvT9",
        "outputId": "db68c08e-69ee-4a52-af39-32723c91a19e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['runs/train/exp/weights/best.pt'], source=/content/yolov5/sam, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.5, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 🚀 v7.0-21-ga1b6e79 Python-3.8.15 torch-1.12.1+cu113 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
            "Traceback (most recent call last):\n",
            "  File \"detect.py\", line 259, in <module>\n",
            "    main(opt)\n",
            "  File \"detect.py\", line 254, in main\n",
            "    run(**vars(opt))\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"detect.py\", line 109, in run\n",
            "    dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)\n",
            "  File \"/content/yolov5/utils/dataloaders.py\", line 251, in __init__\n",
            "    raise FileNotFoundError(f'{p} does not exist')\n",
            "FileNotFoundError: /content/yolov5/sam does not exist\n"
          ]
        }
      ],
      "source": [
        "!python detect.py --weights runs/train/exp/weights/best.pt --img 640 --conf 0.5 --source /content/yolov5/sam\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FayEOOx1TUrg",
        "outputId": "790c758e-82b2-474c-efb5-dc8f05a05dc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 🚀 v7.0-21-ga1b6e79 Python-3.8.15 torch-1.12.1+cu113 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/usr/local/lib/python3.8/dist-packages/pyparsing-3.0.9.dist-info/METADATA'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        }
      ],
      "source": [
        "model = torch.hub.load('.', 'custom', path='runs/train/exp/weights/best.pt', source='local')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_BGcKtUYhFK"
      },
      "outputs": [],
      "source": [
        "dir = '/content/yolov5/sam/'\n",
        "imgs = [dir + f for f in ('17.jpg', 'test.jpg')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KE4N_4KxY88H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "outputId": "054c56c5-8ce4-4823-d070-c0c1bda9fba8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-7580ec361fef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m640\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/yolov5/./models/common.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, ims, size, augment, profile)\u001b[0m\n\u001b[1;32m    682\u001b[0m                 \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'image{i}'\u001b[0m  \u001b[0;31m# filename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# filename or uri\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m                     \u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'http'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    685\u001b[0m                     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexif_transpose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# PIL Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2842\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2843\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2844\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/yolov5/sam/17.jpg'"
          ]
        }
      ],
      "source": [
        "results = model(imgs, size=640)\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xSrUdhyY_cT"
      },
      "outputs": [],
      "source": [
        "results.print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ON1C7b0ZCHG"
      },
      "outputs": [],
      "source": [
        "res = results.pandas().xyxy[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "metadata": {
        "id": "LYH4owgQ2kpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pFwmoFlT7o5"
      },
      "outputs": [],
      "source": [
        "x = cv2.imread('/content/yolov5/runs/detect/exp2/17.jpg')\n",
        "x0, y0, x1, y1, _, _ = results.xyxy[0][0].int()\n",
        "cropped_image = x[y0:y1, x0:x1]\n",
        "cv2_imshow(cropped_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxdVMHHAUGVC"
      },
      "outputs": [],
      "source": [
        "cv2.imwrite('/content/results.jpg', cropped_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3B8v--MTsyR-"
      },
      "outputs": [],
      "source": [
        "test1 = model('/content/t102.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iRhvuwB2Cat"
      },
      "outputs": [],
      "source": [
        "test1.save()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = cv2.imread('/content/yolov5/runs/detect/exp3/t102.jpg')\n",
        "x0, y0, x1, y1, _, _ = test1.xyxy[0][0].int()\n",
        "cropped_image = x[y0:y1, x0:x1]\n",
        "cv2_imshow(cropped_image)"
      ],
      "metadata": {
        "id": "ium6s6ti7re9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv2.imwrite('/content/results.jpg', cropped_image)"
      ],
      "metadata": {
        "id": "sYZfBzJu8gjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove warning message\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "# required library\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "from os.path import splitext,basename\n",
        "from keras.models import model_from_json\n",
        "#from keras.preprocessing.image import load_img, img_to_array\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.mobilenet_v2 import preprocess_input\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import glob"
      ],
      "metadata": {
        "id": "lgPE93Z6-iq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imx = cv2.imread('/content/crop.png')"
      ],
      "metadata": {
        "id": "QosB8ceMCsIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if (len(imx)):\n",
        "    plate_image = cv2.convertScaleAbs(imx[0], alpha=(255.0))\n",
        "\n",
        "    # convert to grayscale and blur the image\n",
        "    gray = cv2.cvtColor(imx, cv2.COLOR_BGR2GRAY)\n",
        "    blur = cv2.GaussianBlur(gray,(7,7),0)\n",
        "\n",
        "    # Applied inversed thresh_binary\n",
        "    binary = cv2.threshold(blur, 180, 255,\n",
        "                         cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
        "    ## Applied dilation\n",
        "    kernel3 = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
        "    thre_mor = cv2.morphologyEx(binary, cv2.MORPH_DILATE, kernel3)"
      ],
      "metadata": {
        "id": "KPM7UFYoPpwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sort_contours(cnts,reverse = False):\n",
        "    i = 0\n",
        "    boundingBoxes = [cv2.boundingRect(c) for c in cnts]\n",
        "    (cnts, boundingBoxes) = zip(*sorted(zip(cnts, boundingBoxes),\n",
        "                                        key=lambda b: b[1][i], reverse=reverse))\n",
        "    return cnts\n",
        "\n",
        "cont, _  = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "# creat a copy version \"test_roi\" of plat_image to draw bounding box\n",
        "test_roi = imx.copy()\n",
        "\n",
        "# Initialize a list which will be used to append charater image\n",
        "crop_characters = []\n",
        "\n",
        "# define standard width and height of character\n",
        "digit_w, digit_h = 28, 28\n",
        "\n",
        "for c in sort_contours(cont):\n",
        "    (x, y, w, h) = cv2.boundingRect(c)\n",
        "    print(x,y,w,h)\n",
        "    ratio = h/w\n",
        "\n",
        "    #print(h/imx.shape[0])\n",
        "    if 0.7<=ratio<=5: # Only select contour with defined ratio\n",
        "        if h/imx.shape[0]>=0.1: # Select contour which has the height larger than 50% of the plate_image\n",
        "            # Draw bounding box arroung digit number\n",
        "            cv2.rectangle(test_roi, (x, y), (x + w, y + h), (0, 255,0), 2)\n",
        "\n",
        "            # Sperate number and give prediction\n",
        "            curr_num = thre_mor[y:y+h,x:x+w]\n",
        "            curr_num = cv2.resize(curr_num, dsize=(digit_w, digit_h))\n",
        "            _, curr_num = cv2.threshold(curr_num, 220, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "            crop_characters.append(curr_num)\n",
        "\n",
        "print(\"Detect {} letters...\".format(len(crop_characters)))\n",
        "fig = plt.figure(figsize=(10,6))\n",
        "plt.axis(False)\n",
        "plt.imshow(test_roi)\n",
        "plt.savefig('grab_digit_contour.png',dpi=300)"
      ],
      "metadata": {
        "id": "OtOVwKxWBfh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(14,4))\n",
        "grid = gridspec.GridSpec(ncols=len(crop_characters),nrows=1,figure=fig)\n",
        "\n",
        "for i in range(len(crop_characters)):\n",
        "    fig.add_subplot(grid[i])\n",
        "    plt.axis(False)\n",
        "    cv2_imshow(crop_characters[i])"
      ],
      "metadata": {
        "id": "iC3ngkTCSAff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "binary = cv2.bitwise_not(binary)\n",
        "cv2_imshow(binary)"
      ],
      "metadata": {
        "id": "JtFGjsHDFx53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "58b6CI1XeFSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model('kesar.h5')\n"
      ],
      "metadata": {
        "id": "f56387ZtfCUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation of the model\n",
        "out = []\n",
        "#im = ((crop_characters[0]))\n",
        "#print(im.shape)\n",
        "#p1 = model.predict(im)\n",
        "for i in range(len(crop_characters)):\n",
        "     print(i)\n",
        "     x = crop_characters[i]\n",
        "     x = np.reshape(x, (28, 28, 1))\n",
        "     #x = np.concatenate([x, x, x], axis=2)\n",
        "     x = np.expand_dims(x, axis=0)\n",
        "     x = np.concatenate([x for _ in range(32)], axis=0)\n",
        "     # print(x.shape)\n",
        "     p1 = model.predict(x)\n",
        "     c=np.argmax(p1)\n",
        "     print (c)\n",
        "     # print(p1)\n",
        "     out.append(c)\n"
      ],
      "metadata": {
        "id": "8CCiz_tPNJks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['0','1','2','3','4','5','6','7','8','9','A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']\n",
        "txt = \"\"\n",
        "for i in (out):\n",
        "  if(i<36):\n",
        "    txt = txt+classes[i]\n",
        "  else:\n",
        "    txt = txt+\"-\"\n",
        "print(txt)"
      ],
      "metadata": {
        "id": "UIU9bCLwe_zD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "92tWt0BPlaG3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOuRdXLeecmk2fw24ocbno/",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}